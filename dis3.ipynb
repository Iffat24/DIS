{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Summer 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Norm of Gradient = 412.472596\n",
      "Iteration 1, Norm of Gradient = 412.472596\n",
      "Iteration 2, Norm of Gradient = 365.010211\n",
      "Iteration 3, Norm of Gradient = 191.200327\n",
      "Iteration 4, Norm of Gradient = 159.279873\n",
      "Iteration 5, Norm of Gradient = 139.025999\n",
      "Iteration 6, Norm of Gradient = 103.474431\n",
      "Iteration 7, Norm of Gradient = 3.332246\n",
      "Iteration 8, Norm of Gradient = 2.146041\n",
      "Iteration 9, Norm of Gradient = 0.523297\n",
      "Iteration 10, Norm of Gradient = 0.304347\n",
      "Iteration 11, Norm of Gradient = 0.473639\n",
      "Iteration 12, Norm of Gradient = 0.856942\n",
      "Iteration 13, Norm of Gradient = 0.594029\n",
      "Iteration 14, Norm of Gradient = 0.228241\n",
      "Iteration 15, Norm of Gradient = 0.033047\n",
      "Iteration 16, Norm of Gradient = 0.017017\n",
      "Iteration 17, Norm of Gradient = 0.013393\n",
      "Iteration 18, Norm of Gradient = 0.012598\n",
      "Iteration 19, Norm of Gradient = 0.006927\n",
      "Iteration 20, Norm of Gradient = 0.009841\n",
      "Iteration 21, Norm of Gradient = 0.014963\n",
      "Iteration 22, Norm of Gradient = 0.003264\n",
      "Iteration 23, Norm of Gradient = 0.003524\n",
      "Iteration 24, Norm of Gradient = 0.009245\n",
      "Iteration 25, Norm of Gradient = 0.009479\n",
      "Iteration 26, Norm of Gradient = 0.012473\n",
      "Iteration 27, Norm of Gradient = 0.008618\n",
      "Iteration 28, Norm of Gradient = 0.001164\n",
      "Iteration 29, Norm of Gradient = 0.001145\n",
      "Iteration 30, Norm of Gradient = 0.005053\n",
      "Iteration 31, Norm of Gradient = 0.008630\n",
      "Iteration 32, Norm of Gradient = 0.004217\n",
      "Iteration 33, Norm of Gradient = 0.000967\n",
      "Iteration 34, Norm of Gradient = 0.002600\n",
      "Iteration 35, Norm of Gradient = 0.020136\n",
      "Iteration 36, Norm of Gradient = 0.019903\n",
      "Iteration 37, Norm of Gradient = 0.003750\n",
      "Iteration 38, Norm of Gradient = 0.002337\n",
      "Iteration 39, Norm of Gradient = 0.005055\n",
      "Iteration 40, Norm of Gradient = 0.015427\n",
      "Iteration 41, Norm of Gradient = 0.011679\n",
      "Iteration 42, Norm of Gradient = 0.008453\n",
      "Iteration 43, Norm of Gradient = 0.003452\n",
      "Iteration 44, Norm of Gradient = 0.005664\n",
      "Iteration 45, Norm of Gradient = 0.040771\n",
      "Iteration 46, Norm of Gradient = 0.004986\n",
      "Iteration 47, Norm of Gradient = 0.013105\n",
      "Iteration 48, Norm of Gradient = 0.071295\n",
      "Iteration 49, Norm of Gradient = 0.080158\n",
      "Iteration 50, Norm of Gradient = 0.042928\n",
      "Iteration 51, Norm of Gradient = 0.075237\n",
      "Iteration 52, Norm of Gradient = 0.186652\n",
      "Iteration 53, Norm of Gradient = 0.192429\n",
      "Iteration 54, Norm of Gradient = 0.195406\n",
      "Iteration 55, Norm of Gradient = 0.163596\n",
      "Iteration 56, Norm of Gradient = 0.177916\n",
      "Iteration 57, Norm of Gradient = 0.503235\n",
      "Iteration 58, Norm of Gradient = 0.254885\n",
      "Iteration 59, Norm of Gradient = 0.171206\n",
      "Iteration 60, Norm of Gradient = 0.079226\n",
      "Iteration 61, Norm of Gradient = 0.067031\n",
      "Iteration 62, Norm of Gradient = 0.158277\n",
      "Iteration 63, Norm of Gradient = 0.041787\n",
      "Iteration 64, Norm of Gradient = 0.011865\n",
      "Iteration 65, Norm of Gradient = 0.020821\n",
      "Iteration 66, Norm of Gradient = 0.036946\n",
      "Iteration 67, Norm of Gradient = 0.000853\n",
      "Iteration 68, Norm of Gradient = 0.001729\n",
      "Iteration 69, Norm of Gradient = 0.000439\n",
      "Iteration 70, Norm of Gradient = 0.001717\n",
      "Iteration 71, Norm of Gradient = 0.001062\n",
      "Iteration 72, Norm of Gradient = 0.002155\n",
      "Iteration 73, Norm of Gradient = 0.003095\n",
      "Iteration 74, Norm of Gradient = 0.002927\n",
      "Iteration 75, Norm of Gradient = 0.001863\n",
      "Iteration 76, Norm of Gradient = 0.001032\n",
      "Iteration 77, Norm of Gradient = 0.000406\n",
      "Iteration 78, Norm of Gradient = 0.001074\n",
      "Iteration 79, Norm of Gradient = 0.001832\n",
      "Iteration 80, Norm of Gradient = 0.001280\n",
      "Iteration 81, Norm of Gradient = 0.000577\n",
      "Iteration 82, Norm of Gradient = 0.002165\n",
      "Iteration 83, Norm of Gradient = 0.015497\n",
      "Iteration 84, Norm of Gradient = 0.019601\n",
      "Iteration 85, Norm of Gradient = 0.005293\n",
      "Iteration 86, Norm of Gradient = 0.006226\n",
      "Iteration 87, Norm of Gradient = 0.006381\n",
      "Iteration 88, Norm of Gradient = 0.014973\n",
      "Iteration 89, Norm of Gradient = 0.038551\n",
      "Iteration 90, Norm of Gradient = 0.105531\n",
      "Iteration 91, Norm of Gradient = 0.035577\n",
      "Iteration 92, Norm of Gradient = 0.037976\n",
      "Iteration 93, Norm of Gradient = 0.065444\n",
      "Iteration 94, Norm of Gradient = 0.062632\n",
      "Iteration 95, Norm of Gradient = 0.100324\n",
      "Iteration 96, Norm of Gradient = 0.051559\n",
      "Iteration 97, Norm of Gradient = 0.039519\n",
      "Iteration 98, Norm of Gradient = 0.110023\n",
      "Iteration 99, Norm of Gradient = 0.090583\n",
      "Iteration 100, Norm of Gradient = 0.026646\n",
      "Iteration 101, Norm of Gradient = 0.003366\n",
      "Iteration 102, Norm of Gradient = 0.001527\n",
      "Iteration 103, Norm of Gradient = 0.001529\n",
      "Iteration 104, Norm of Gradient = 0.002696\n",
      "Iteration 105, Norm of Gradient = 0.001787\n",
      "Iteration 106, Norm of Gradient = 0.000765\n",
      "Iteration 107, Norm of Gradient = 0.000287\n",
      "Iteration 108, Norm of Gradient = 0.000544\n",
      "Iteration 109, Norm of Gradient = 0.000886\n",
      "Iteration 110, Norm of Gradient = 0.000304\n",
      "Iteration 111, Norm of Gradient = 0.000116\n",
      "Iteration 112, Norm of Gradient = 0.000336\n",
      "Iteration 113, Norm of Gradient = 0.000968\n",
      "Iteration 114, Norm of Gradient = 0.000226\n",
      "Iteration 115, Norm of Gradient = 0.000765\n",
      "Iteration 116, Norm of Gradient = 0.001970\n",
      "Iteration 117, Norm of Gradient = 0.013216\n",
      "Iteration 118, Norm of Gradient = 0.034636\n",
      "Iteration 119, Norm of Gradient = 0.017807\n",
      "Iteration 120, Norm of Gradient = 0.009943\n",
      "Iteration 121, Norm of Gradient = 0.067956\n",
      "Iteration 122, Norm of Gradient = 0.059229\n",
      "Iteration 123, Norm of Gradient = 0.033687\n",
      "Iteration 124, Norm of Gradient = 0.016856\n",
      "Iteration 125, Norm of Gradient = 0.008329\n",
      "Iteration 126, Norm of Gradient = 0.004247\n",
      "Iteration 127, Norm of Gradient = 0.002220\n",
      "Iteration 128, Norm of Gradient = 0.001204\n",
      "Iteration 129, Norm of Gradient = 0.000495\n",
      "Iteration 130, Norm of Gradient = 0.001335\n",
      "Iteration 131, Norm of Gradient = 0.001471\n",
      "Iteration 132, Norm of Gradient = 0.001620\n",
      "Iteration 133, Norm of Gradient = 0.000834\n",
      "Iteration 134, Norm of Gradient = 0.000125\n",
      "Iteration 135, Norm of Gradient = 0.000095\n",
      "Iteration 136, Norm of Gradient = 0.000357\n",
      "Iteration 137, Norm of Gradient = 0.000392\n",
      "Iteration 138, Norm of Gradient = 0.000150\n",
      "Iteration 139, Norm of Gradient = 0.000043\n",
      "Iteration 140, Norm of Gradient = 0.000225\n",
      "Iteration 141, Norm of Gradient = 0.000669\n",
      "Iteration 142, Norm of Gradient = 0.001822\n",
      "Iteration 143, Norm of Gradient = 0.001228\n",
      "Iteration 144, Norm of Gradient = 0.000318\n",
      "Iteration 145, Norm of Gradient = 0.000325\n",
      "Iteration 146, Norm of Gradient = 0.001343\n",
      "Iteration 147, Norm of Gradient = 0.002846\n",
      "Iteration 148, Norm of Gradient = 0.002574\n",
      "Iteration 149, Norm of Gradient = 0.001600\n",
      "Iteration 150, Norm of Gradient = 0.007710\n",
      "Iteration 151, Norm of Gradient = 0.035963\n",
      "Iteration 152, Norm of Gradient = 0.011048\n",
      "Iteration 153, Norm of Gradient = 0.004470\n",
      "Iteration 154, Norm of Gradient = 0.004806\n",
      "Iteration 155, Norm of Gradient = 0.009499\n",
      "Iteration 156, Norm of Gradient = 0.006021\n",
      "Iteration 157, Norm of Gradient = 0.001622\n",
      "Iteration 158, Norm of Gradient = 0.000167\n",
      "Iteration 159, Norm of Gradient = 0.000104\n",
      "Iteration 160, Norm of Gradient = 0.000686\n",
      "Iteration 161, Norm of Gradient = 0.000258\n",
      "Iteration 162, Norm of Gradient = 0.000080\n",
      "Iteration 163, Norm of Gradient = 0.000048\n",
      "Iteration 164, Norm of Gradient = 0.000065\n",
      "Iteration 165, Norm of Gradient = 0.000186\n",
      "Iteration 166, Norm of Gradient = 0.000399\n",
      "Iteration 167, Norm of Gradient = 0.000275\n",
      "Iteration 168, Norm of Gradient = 0.000161\n",
      "Iteration 169, Norm of Gradient = 0.000074\n",
      "Iteration 170, Norm of Gradient = 0.000039\n",
      "Iteration 171, Norm of Gradient = 0.000029\n",
      "Iteration 172, Norm of Gradient = 0.000038\n",
      "Iteration 173, Norm of Gradient = 0.000170\n",
      "Iteration 174, Norm of Gradient = 0.000237\n",
      "Iteration 175, Norm of Gradient = 0.000205\n",
      "Iteration 176, Norm of Gradient = 0.000066\n",
      "Iteration 177, Norm of Gradient = 0.000020\n",
      "Iteration 178, Norm of Gradient = 0.000022\n",
      "Iteration 179, Norm of Gradient = 0.000094\n",
      "Iteration 180, Norm of Gradient = 0.000154\n",
      "Iteration 181, Norm of Gradient = 0.000039\n",
      "Iteration 182, Norm of Gradient = 0.000026\n",
      "Iteration 183, Norm of Gradient = 0.000107\n",
      "Iteration 184, Norm of Gradient = 0.000371\n",
      "Iteration 185, Norm of Gradient = 0.000102\n",
      "Iteration 186, Norm of Gradient = 0.000073\n",
      "Iteration 187, Norm of Gradient = 0.000257\n",
      "Iteration 188, Norm of Gradient = 0.001141\n",
      "Iteration 189, Norm of Gradient = 0.000814\n",
      "Iteration 190, Norm of Gradient = 0.000258\n",
      "Iteration 191, Norm of Gradient = 0.000148\n",
      "Iteration 192, Norm of Gradient = 0.000197\n",
      "Iteration 193, Norm of Gradient = 0.000568\n",
      "Iteration 194, Norm of Gradient = 0.001988\n",
      "Iteration 195, Norm of Gradient = 0.001696\n",
      "Iteration 196, Norm of Gradient = 0.000573\n",
      "Iteration 197, Norm of Gradient = 0.000260\n",
      "Iteration 198, Norm of Gradient = 0.000325\n",
      "Iteration 199, Norm of Gradient = 0.000858\n",
      "Iteration 200, Norm of Gradient = 0.002371\n",
      "Iteration 201, Norm of Gradient = 0.000462\n",
      "Iteration 202, Norm of Gradient = 0.000688\n",
      "Iteration 203, Norm of Gradient = 0.001575\n",
      "Iteration 204, Norm of Gradient = 0.002906\n",
      "Iteration 205, Norm of Gradient = 0.004122\n",
      "Iteration 206, Norm of Gradient = 0.002508\n",
      "Iteration 207, Norm of Gradient = 0.000413\n",
      "Iteration 208, Norm of Gradient = 0.004825\n",
      "Iteration 209, Norm of Gradient = 0.004359\n",
      "Iteration 210, Norm of Gradient = 0.001283\n",
      "Iteration 211, Norm of Gradient = 0.000405\n",
      "Iteration 212, Norm of Gradient = 0.000440\n",
      "Iteration 213, Norm of Gradient = 0.001915\n",
      "Iteration 214, Norm of Gradient = 0.002648\n",
      "Iteration 215, Norm of Gradient = 0.000902\n",
      "Iteration 216, Norm of Gradient = 0.000788\n",
      "Iteration 217, Norm of Gradient = 0.000448\n",
      "Iteration 218, Norm of Gradient = 0.000530\n",
      "Iteration 219, Norm of Gradient = 0.004517\n",
      "Iteration 220, Norm of Gradient = 0.004064\n",
      "Iteration 221, Norm of Gradient = 0.001195\n",
      "Iteration 222, Norm of Gradient = 0.000413\n",
      "Iteration 223, Norm of Gradient = 0.000465\n",
      "Iteration 224, Norm of Gradient = 0.001211\n",
      "Iteration 225, Norm of Gradient = 0.001858\n",
      "Iteration 226, Norm of Gradient = 0.000762\n",
      "Iteration 227, Norm of Gradient = 0.000208\n",
      "Iteration 228, Norm of Gradient = 0.000376\n",
      "Iteration 229, Norm of Gradient = 0.000623\n",
      "Iteration 230, Norm of Gradient = 0.000417\n",
      "Iteration 231, Norm of Gradient = 0.000137\n",
      "Iteration 232, Norm of Gradient = 0.000098\n",
      "Iteration 233, Norm of Gradient = 0.000373\n",
      "Iteration 234, Norm of Gradient = 0.000266\n",
      "Iteration 235, Norm of Gradient = 0.000049\n",
      "Iteration 236, Norm of Gradient = 0.000082\n",
      "Iteration 237, Norm of Gradient = 0.000241\n",
      "Iteration 238, Norm of Gradient = 0.000293\n",
      "Iteration 239, Norm of Gradient = 0.000123\n",
      "Iteration 240, Norm of Gradient = 0.000065\n",
      "Iteration 241, Norm of Gradient = 0.000023\n",
      "Iteration 242, Norm of Gradient = 0.000052\n",
      "Iteration 243, Norm of Gradient = 0.000146\n",
      "Iteration 244, Norm of Gradient = 0.000029\n",
      "---------------------------\n",
      " E0 = -78.652196\n",
      " B0 = 0.005853\n",
      " V0 = 272.507556\n",
      " B0_prime = 4.820467\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data for V and E\n",
    "V = np.array([211.24813, 240.01760, 271.28726, 305.16127, 341.74381, 381.13906])\n",
    "E = np.array([-78.58421, -78.63681, -78.65265, -78.64328, -78.61798, -78.58299])\n",
    "\n",
    "# Initial conditions\n",
    "E0 = -78.65265\n",
    "B0 = 0.5\n",
    "V0 = 271.28726\n",
    "B0_prime = 0.5\n",
    "x0 = np.array([E0, B0, V0, B0_prime])\n",
    "\n",
    "# Function for the Murnaghan equation (equation 8.1)\n",
    "def fun_E(V, x):\n",
    "    E0, B0, V0, B0_prime = x\n",
    "    term1 = (V / V0) ** (1 - B0_prime) / (B0_prime * (B0_prime - 1))\n",
    "    term2 = V / (B0_prime * V0) - 1 / (B0_prime - 1)\n",
    "    return E0 + B0 * V0 * (term1 + term2)\n",
    "\n",
    "# Objective function to minimize\n",
    "def fun_opt(E, V, x):\n",
    "    E_V = fun_E(V, x)\n",
    "    return np.sum((E_V - E) ** 2)\n",
    "\n",
    "# Central difference gradient calculation\n",
    "def calc_gradient(E, V, x, step=1e-5):\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_forward = x.copy()\n",
    "        x_backward = x.copy()\n",
    "        x_forward[i] += step\n",
    "        x_backward[i] -= step\n",
    "        grad[i] = (fun_opt(E, V, x_forward) - fun_opt(E, V, x_backward)) / (2 * step)\n",
    "    return grad\n",
    "\n",
    "# Backtracking line search to satisfy Wolfe conditions\n",
    "def back_track(E, V, xk, pk, grad, fun, alpha=1.0, rho=0.5, gamma=1e-4):\n",
    "    while fun(E, V, xk + alpha * pk) > fun(E, V, xk) + gamma * alpha * np.dot(grad, pk):\n",
    "        alpha *= rho\n",
    "    return alpha\n",
    "\n",
    "# Initial step using steepest descent\n",
    "p0 = -calc_gradient(E, V, x0)\n",
    "alpha = back_track(E, V, x0, p0, calc_gradient(E, V, x0), fun_opt)\n",
    "xk = x0 + alpha * p0\n",
    "s0 = p0\n",
    "\n",
    "# Iterative optimization using nonlinear conjugate gradient (Fletcher-Reeves)\n",
    "tolerance = 1e-5\n",
    "max_iter = 1000\n",
    "counter = 0\n",
    "residual = np.linalg.norm(calc_gradient(E, V, xk))\n",
    "\n",
    "while residual > tolerance:\n",
    "    print(f\"Iteration {counter}, Norm of Gradient = {residual:.6f}\")\n",
    "\n",
    "    # Calculate new gradient and update direction using Fletcher-Reeves\n",
    "    pk = -calc_gradient(E, V, xk)\n",
    "    beta = np.dot(pk, pk) / np.dot(p0, p0)\n",
    "    sk = pk + beta * s0\n",
    "\n",
    "    # Line search using backtracking\n",
    "    alpha = back_track(E, V, xk, sk, calc_gradient(E, V, xk), fun_opt)\n",
    "\n",
    "    # Update variables\n",
    "    xk = xk + alpha * sk\n",
    "    p0 = pk\n",
    "    s0 = sk\n",
    "    residual = np.linalg.norm(pk)\n",
    "    counter += 1\n",
    "\n",
    "    if counter >= max_iter:\n",
    "        print(\"MAX_ITER REACHED!\")\n",
    "        break\n",
    "\n",
    "# Display results\n",
    "print(\"---------------------------\")\n",
    "print(f\" E0 = {xk[0]:.6f}\")\n",
    "print(f\" B0 = {xk[1]:.6f}\")\n",
    "print(f\" V0 = {xk[2]:.6f}\")\n",
    "print(f\" B0_prime = {xk[3]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
